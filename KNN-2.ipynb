{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cdaa396-2b01-4bd5-a2d5-c26cdae341b0",
   "metadata": {},
   "source": [
    "Q1. What is the main difference between the Euclidean distance metric and the Manhattan distance\n",
    "metric in KNN? How might this difference affect the performance of a KNN classifier or regressor?\n",
    "Ans:-Euclidean Distance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b256856c-13d9-457b-8b65-7d0beb45a860",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def euclidean_distance(point1, point2):\n",
    "    return np.sqrt(np.sum((point1 - point2)**2))\n",
    "\n",
    "# Example usage\n",
    "point1 = np.array([1, 2])\n",
    "point2 = np.array([4, 6])\n",
    "\n",
    "euclidean_distance_value = euclidean_distance(point1, point2)\n",
    "print(\"Euclidean Distance:\", euclidean_distance_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5525c1-d463-4389-b771-b03d2b06b249",
   "metadata": {},
   "outputs": [],
   "source": [
    "Manhattan Distance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4e5eb4-0a2a-4fb8-8826-a6f80143a302",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def manhattan_distance(point1, point2):\n",
    "    return np.sum(np.abs(point1 - point2))\n",
    "\n",
    "# Example usage\n",
    "point1 = np.array([1, 2])\n",
    "point2 = np.array([4, 6])\n",
    "\n",
    "manhattan_distance_value = manhattan_distance(point1, point2)\n",
    "print(\"Manhattan Distance:\", manhattan_distance_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee85805-cc8c-4fa8-8a95-1548d827a3b1",
   "metadata": {},
   "source": [
    "Effect on KNN:\n",
    "Sensitivity to Distance Metrics:\n",
    "\n",
    "KNN's performance can be sensitive to the choice of distance metric.\n",
    "Euclidean distance is sensitive to the overall distance between points in a straight line.\n",
    "Manhattan distance is sensitive to the sum of the absolute differences along each dimension.\n",
    "Feature Relationships:\n",
    "\n",
    "Euclidean distance may be more suitable when feature relationships are well-behaved and continuous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e00012c-63a8-4801-96d4-a101b5157c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Assuming X_train, X_test, y_train, y_test are your training and testing sets\n",
    "\n",
    "# Euclidean Distance\n",
    "knn_euclidean = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n",
    "knn_euclidean.fit(X_train, y_train)\n",
    "y_pred_euclidean = knn_euclidean.predict(X_test)\n",
    "accuracy_euclidean = accuracy_score(y_test, y_pred_euclidean)\n",
    "print(\"Accuracy with Euclidean Distance:\", accuracy_euclidean)\n",
    "\n",
    "# Manhattan Distance\n",
    "knn_manhattan = KNeighborsClassifier(n_neighbors=5, metric='manhattan')\n",
    "knn_manhattan.fit(X_train, y_train)\n",
    "y_pred_manhattan = knn_manhattan.predict(X_test)\n",
    "accuracy_manhattan = accuracy_score(y_test, y_pred_manhattan)\n",
    "print(\"Accuracy with Manhattan Distance:\", accuracy_manhattan)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e47d5a-8891-4ae0-81f2-3f6e668b1756",
   "metadata": {},
   "source": [
    "Q2. How do you choose the optimal value of k for a KNN classifier or regressor? What techniques can be\n",
    "used to determine the optimal k value?\n",
    "Ans:-1. Grid Search:\r\n",
    "Perform a grid search over a range of\n",
    "�\r\n",
    "k values and evaluate the model's performance using cross-validation. Choose t \r\n",
    "�\r\n",
    "k value that results in the best performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd093b7c-c71c-438c-b51a-86a03dcc6e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load iris dataset as an example\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Define the parameter grid (here, we choose k from 1 to 10)\n",
    "param_grid = {'n_neighbors': range(1, 11)}\n",
    "\n",
    "# Instantiate KNN classifier\n",
    "knn_classifier = KNeighborsClassifier()\n",
    "\n",
    "# Perform grid search with 5-fold cross-validation\n",
    "grid_search = GridSearchCV(knn_classifier, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Get the best k value from the grid search\n",
    "best_k = grid_search.best_params_['n_neighbors']\n",
    "print(\"Best k value:\", best_k)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1454662-8b65-417b-ac43-10781b7ce6b6",
   "metadata": {},
   "source": [
    "2. Elbow Method:\n",
    "Plot the model's performance (e.g., accuracy or error) for different \n",
    "k values and observe the point where the performance starts to plateau. This is often referred to as the \"elbow\" of the curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040f1cfe-7bbb-4358-b6d9-c1614222c4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assume X_train, X_test, y_train, y_test are your training and testing sets\n",
    "\n",
    "# Define a range of k values\n",
    "k_values = range(1, 21)\n",
    "\n",
    "# Initialize lists to store accuracy values\n",
    "accuracy_values = []\n",
    "\n",
    "# Test different k values\n",
    "for k in k_values:\n",
    "    knn_classifier = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn_classifier.fit(X_train, y_train)\n",
    "    accuracy = knn_classifier.score(X_test, y_test)\n",
    "    accuracy_values.append(accuracy)\n",
    "\n",
    "# Plot the accuracy values for different k values\n",
    "plt.plot(k_values, accuracy_values, marker='o')\n",
    "plt.xlabel('k (Number of Neighbors)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('KNN Classifier: Elbow Method')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d30ac0-8592-46c3-84e7-256a9a8c6c00",
   "metadata": {},
   "source": [
    "Q3. How does the choice of distance metric affect the performance of a KNN classifier or regressor? In\n",
    "what situations might you choose one distance metric over the other?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b545b7e4-e161-4f17-98f0-af052f664d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load Iris dataset as an example\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Instantiate KNN classifier with Euclidean distance\n",
    "knn_euclidean = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n",
    "knn_euclidean.fit(X_train, y_train)\n",
    "y_pred_euclidean = knn_euclidean.predict(X_test)\n",
    "accuracy_euclidean = accuracy_score(y_test, y_pred_euclidean)\n",
    "print(\"Accuracy with Euclidean Distance:\", accuracy_euclidean)\n",
    "\n",
    "# Instantiate KNN classifier with Manhattan distance\n",
    "knn_manhattan = KNeighborsClassifier(n_neighbors=5, metric='manhattan')\n",
    "knn_manhattan.fit(X_train, y_train)\n",
    "y_pred_manhattan = knn_manhattan.predict(X_test)\n",
    "accuracy_manhattan = accuracy_score(y_test, y_pred_manhattan)\n",
    "print(\"Accuracy with Manhattan Distance:\", accuracy_manhattan)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57cc2a2-1ed4-4469-8961-552abb64acd5",
   "metadata": {},
   "source": [
    "Q4. What are some common hyperparameters in KNN classifiers and regressors, and how do they affect\n",
    "the performance of the model? How might you go about tuning these hyperparameters to improve\n",
    "model performance?\n",
    "Ans:-K-Nearest Neighbors (KNN) classifiers and regressors have several hyperparameters that can impact the performance of the model. Understanding these hyperparameters and tuning them appropriately is essential for achieving the best results. Here are some common hyperparameters in KNN models and their effects on performance:\r\n",
    "\r\n",
    "Common Hyperparameters in KNN:\r\n",
    "Number of Neighbor\r\n",
    "�\r",
    "k):\r\n",
    "\r\n",
    "Effect: Determines the number of nearest neighbors considered for classification or regression.\r\n",
    "Tuning: Use techniques like grid search or cross-validation to find the timal \r\n",
    "�\r\n",
    "k value based on the dataset characteristicsSmaller \r\n",
    "�\r\n",
    "k values may lead to overfitting, while larger values may result in underfitting.\r\n",
    "Distance Metric:\r\n",
    "\r\n",
    "Effect: Defines the measure of distance between data points (e.g., Euclidean, Manhattan).\r\n",
    "Tuning: Experiment with different distance metrics based on the nature of the data. Some problems may benefit from Euclidean distance, while others may prefer Manhattan or other metrics. Choose the one that aligns with the underlying relationships in the data.\r\n",
    "Weighting of Neighbors:\r\n",
    "\r\n",
    "Effect: Determines whether all neighbors contribute equally or are weighted by their distance.\r\n",
    "Tuning: Set the weights hyperparameter. Options typically include 'uniform' (equal weighting) or 'distance' (weighting inversely proportional to distance). Weighting by distance is often beneficial when neighbors closer to the query point are considered more influential.\r\n",
    "Algorithm for Nearest Neighbors Search:\r\n",
    "\r\n",
    "Effect: Specifies the algorithm used to find the nearest neighbors (e.g., 'brute', 'kd_tree', 'ball_tree', 'auto').\r\n",
    "Tuning: Depending on the dataset size and dimensionality, different algorithms may be more efficient. For smaller datasets, 'brute' force may suffice, while larger datasets may benefit from tree-based methods. The 'auto' option selects the most suitable algorithm based on the input data.\r\n",
    "Leaf Size (for tree-based algorithms):\r\n",
    "\r\n",
    "Effect: Determines the number of points in a leaf node of the KD tree or Ball tree.\r\n",
    "Tuning: Adjust the leaf_size parameter based on the dataset size and dimensionality. Smaller values may result in a more accurate but slower tree construction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d539b9-e50f-4f76-9f0c-ab760c91f993",
   "metadata": {},
   "source": [
    "Q5. How does the size of the training set affect the performance of a KNN classifier or regressor? What\n",
    "techniques can be used to optimize the size of the training set?\n",
    "Ans:-The size of the training set can significantly impact the performance of a K-Nearest Neighbors (KNN) classifier or regressor. The relationship between the training set size and model performance is influenced by various factors. Here's an overview of how training set size affects KNN models and techniques to optimize the size:\r\n",
    "\r\n",
    "Impact of Training Set Size:\r\n",
    "Smaller Training Sets:\r\n",
    "\r\n",
    "Pros:\r\n",
    "Faster model training.\r\n",
    "Less computational resources required.\r\n",
    "May be suitable for simpler or less complex problems.\r\n",
    "Cons:\r\n",
    "Prone to overfitting, especially with smaller \r\n",
    "�\r\n",
    "k values.\r\n",
    "Generalization to unseen data may be limited.\r\n",
    "Larger Training Sets:\r\n",
    "\r\n",
    "Pros:\r\n",
    "Improved generalization to unseen data.\r\n",
    "Reduced risk of overfitting.\r\n",
    "More representative of the underlying data distribution.\r\n",
    "Cons:\r\n",
    "Increased computational cost during training and prediction.\r\n",
    "Techniques to Optimize Training Set Size:\r\n",
    "Cross-Validation:\r\n",
    "\r\n",
    "Use cross-validation techniques to assess the model's performance across different training set sizes.\r\n",
    "Helps identify the trade-off between bias and variance.\r\n",
    "Learning Curves:\r\n",
    "\r\n",
    "Plot learning curves to visualize the model's performance with varying training set sizes.\r\n",
    "Observe how the training and validation performance change as the training set size increases.\r\n",
    "Incremental Learning:\r\n",
    "\r\n",
    "Consider incremental learning or online learning approaches where the model is updated as new data becomes available.\r\n",
    "Suitable for scenarios where data arrives in batches or streams.\r\n",
    "Feature Importance and Selection:\r\n",
    "\r\n",
    "Assess the importance of features and focus on those that contribute significantly to the model's performance.\r\n",
    "Reducing the dimensionality of the feature space can make smaller training sets more effective.\r\n",
    "Data Augmentation:\r\n",
    "\r\n",
    "Augment the training set by applying transformations or generating synthetic samples.\r\n",
    "Increases the diversity of the training data without collecting additional real-world samples.\r\n",
    "Active Learning:\r\n",
    "\r\n",
    "Use active learning strategies to selectively query and label instances that are most informative to the model.\r\n",
    "Can help in situations where labeling data is resource-intensive.\r\n",
    "Bootstrapping:\r\n",
    "\r\n",
    "Apply bootstrapping techniques to generate multiple samples from the existing training set.\r\n",
    "Can be useful when working with limited labeled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ff7b27-9b1e-4518-a57f-9cd2165c99f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Assume X, y are your feature matrix and labels\n",
    "train_sizes, train_scores, val_scores = learning_curve(\n",
    "    KNeighborsClassifier(n_neighbors=5), X, y, cv=5, scoring='accuracy', train_sizes=np.linspace(0.1, 1.0, 10)\n",
    ")\n",
    "\n",
    "# Plot learning curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(train_sizes, np.mean(train_scores, axis=1), label='Training Score')\n",
    "plt.plot(train_sizes, np.mean(val_scores, axis=1), label='Validation Score')\n",
    "plt.xlabel('Training Set Size')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Learning Curve for KNN Classifier')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7fdeda2-9e1c-4e2d-bfaa-ca1dc386c441",
   "metadata": {},
   "source": [
    "Q6. What are some potential drawbacks of using KNN as a classifier or regressor? How might you\n",
    "overcome these drawbacks to improve the performance of the model?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
